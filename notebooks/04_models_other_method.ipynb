{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "215db92a",
   "metadata": {},
   "source": [
    "# LightGBM Method\n",
    "\n",
    "__Our third method will be a Light Gradient Boosting Machine__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b067811a",
   "metadata": {},
   "source": [
    "This model was originaly created by Microsoft. It basically is a bunch of many small decision trees, build one after another, where each tree tries to correct the mistakes of the preious ones. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3108f6",
   "metadata": {},
   "source": [
    "We are going to trin a LightGBM model (Poisson) on the dataset,, we will evaluate with Poisson deviance, RMSE/MAE, and show calliration.\n",
    "\n",
    "__First the libraries__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e489e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb #the library of our chosen method\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split #we already have two sets, but with this tool we will get a validation set before the test set\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf891ef",
   "metadata": {},
   "source": [
    "__Now we code the poisson deviance and evaluation metrics__\n",
    "\n",
    "This are just small utility functions, just to help us a bit with this.\n",
    "\n",
    "Because of the nature of the __LightGBM__ method, Poisson deviance becomes a very usefull tool. \n",
    "\n",
    "Counts are different from continous values, Poisson deviance measures how well predicted expected counts exxplain the boserved counts, paying attention to the relative scale - _error when tru counts are large are treated differently than errors when counts are small_.\n",
    "\n",
    "\n",
    "By computing Poisson deviance we can compare our model to what it actually optimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a68a89cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def poisson_deviance(y_true, y_pred, eps=1e-9):\n",
    "    \"\"\"\n",
    "    Compute the mean Poisson deviance between true counts and predicted expected counts.\n",
    "    y_true: observed counts\n",
    "    y_pred: predicted (what we expect) counts (more or equal to 0)\n",
    "    eps    : small value to avoid log(0)\n",
    "    \"\"\"\n",
    "    y_pred = np.maximum(y_pred, eps)\n",
    "    y_true = np.asarray(y_true)\n",
    "    term = y_true * np.log((y_true + eps) / y_pred) - (y_true - y_pred)\n",
    "\n",
    "    return 2.0 * np.mean(term)\n",
    "\n",
    "def evaluate_counts(y_true, y_pred, exposure = None):\n",
    "    \"\"\"\n",
    "    Evaluate predicted counts vs true counts.\n",
    "    IF exposure is provided, predictons/true sould be counts.\n",
    "    Return dict whit RMSE, MAE, Poisson deviance, total observed vs predicted.\n",
    "    \n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"rmse\": np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "        \"mae\": mean_absolute_error(y_true, y_pred),\n",
    "        \"poisson_deviance\": poisson_deviance(y_true, y_pred),\n",
    "        \"total_true\": float(np.sum(y_true)),\n",
    "        \"total_pred\": float(np.sum(y_pred))\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43a5e43",
   "metadata": {},
   "source": [
    "__Then we load and clean the data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab66191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (542410, 12)\n",
      "Test shape: (135603, 12)\n",
      "\n",
      "Train columns: ['IDpol', 'ClaimNb', 'Exposure', 'Area', 'VehPower', 'VehAge', 'DrivAge', 'BonusMalus', 'VehBrand', 'VehGas', 'Density', 'Region']\n",
      "Rows with Exposure > 1: 994 (0.18% of dataset)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Exposure_orig</th>\n",
       "      <th>Exposure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>1.03</td>\n",
       "      <td>1.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1298</th>\n",
       "      <td>1.01</td>\n",
       "      <td>1.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1767</th>\n",
       "      <td>1.06</td>\n",
       "      <td>1.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990</th>\n",
       "      <td>1.01</td>\n",
       "      <td>1.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2209</th>\n",
       "      <td>1.03</td>\n",
       "      <td>1.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2667</th>\n",
       "      <td>1.03</td>\n",
       "      <td>1.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3380</th>\n",
       "      <td>1.10</td>\n",
       "      <td>1.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4342</th>\n",
       "      <td>1.13</td>\n",
       "      <td>1.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4555</th>\n",
       "      <td>1.05</td>\n",
       "      <td>1.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4844</th>\n",
       "      <td>1.02</td>\n",
       "      <td>1.02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Exposure_orig  Exposure\n",
       "272            1.03      1.03\n",
       "1298           1.01      1.01\n",
       "1767           1.06      1.06\n",
       "1990           1.01      1.01\n",
       "2209           1.03      1.03\n",
       "2667           1.03      1.03\n",
       "3380           1.10      1.10\n",
       "4342           1.13      1.13\n",
       "4555           1.05      1.05\n",
       "4844           1.02      1.02"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with Exposure > 1 (capped): 994\n",
      "Numeric columns: ['VehAge', 'DrivAge', 'BonusMalus', 'Density', 'log_exposure']\n",
      "Categorical columns: ['Area', 'VehPower', 'VehBrand', 'VehGas', 'Region', 'exposure_large']\n",
      "Total features used: 11\n"
     ]
    }
   ],
   "source": [
    "# --- LOAD THE DATA --- #\n",
    "\n",
    "train = pd.read_csv(\"../data/claims_train.csv\")\n",
    "test = pd.read_csv(\"../data/claims_test.csv\")\n",
    "\n",
    "print(\"Train shape:\", train.shape)\n",
    "print(\"Test shape:\", test.shape)\n",
    "print(\"\\nTrain columns:\", train.columns.tolist())\n",
    "\n",
    "train = train.copy() #just in case\n",
    "\n",
    "\n",
    "# --- ABOUT THE EXPOSURE FEATURE --- #\n",
    "\n",
    "#so we got an email from Gabriel that told us how exposure significantly greater than 1 can be suspicious (entry mistakes, policies observed for longer periods or duplaces), so we are capping it at 1, prevvents a few large-exposure rows from skewing the model. \n",
    "train['Exposure_orig'] = train['Exposure']\n",
    "train['exposure_large'] = (train['Exposure'] > 1).astype(int) #new column with the exposures that are above 1\n",
    "\n",
    "num_large = int(train['exposure_large'].sum())\n",
    "print(f\"Rows with Exposure > 1: {num_large} ({num_large/len(train):.2%} of dataset)\")\n",
    "if num_large > 0:\n",
    "    display(train.loc[train['exposure_large'] == 1, ['Exposure_orig', 'Exposure']].head(10))\n",
    "\n",
    "train['Exposure'] = train['Exposure'].clip(upper=1.0)\n",
    "\n",
    "print(\"Rows with Exposure > 1 (capped):\", int(train['exposure_large'].sum()))\n",
    "\n",
    "\n",
    "# --- SELECTING THE DATA --- #\n",
    "\n",
    "target = 'ClaimNb'\n",
    "exposure = 'Exposure'\n",
    "#we are going to exlude both target and exposure as well as ID for the teatures\n",
    "\n",
    "exclude = {target, 'Exposure_orig', exposure, 'IDpol', 'Id', 'PolicyID'}\n",
    "features = [c for c in train.columns if c not in exclude and c != target]#we select the colums we want the model to learn from basically\n",
    "\n",
    "train['log_exposure'] = np.log(train[exposure].replace(0, 1e-6)) #we create a new colum that basically makes exposure a numeric value centered around 0:\n",
    "    # when exposure = 1   --> log_exposure = 0\n",
    "    # when exposure = 0.5 --> log_exposure = -0.693\n",
    "    # when exposure = 2   --> log_exposure = 0.693\n",
    "\n",
    "if 'log_exposure' not in features:\n",
    "    features.append('log_exposure')\n",
    "if 'exposure_large' not in features:\n",
    "    features.append('exposure_large')\n",
    "#we make sure to append those to the features\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#now we just identify the colums this is just to visualize it better dw twin\n",
    "cat_cols = [c for c in features if (train[c].dtype == 'object' or train[c].nunique() <= 50)]\n",
    "num_cols = [c for c in features if c not in cat_cols]\n",
    "\n",
    "# print that shit\n",
    "print(\"Numeric columns:\", num_cols)\n",
    "print(\"Categorical columns:\", cat_cols)\n",
    "print(\"Total features used:\", len(features))\n",
    "\n",
    "\n",
    "#kinda the whole goal of this cell was to see which colums of the table will the model look at and in what form, real long tho, boring as well "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e51b19",
   "metadata": {},
   "source": [
    "__Now, we Encode categorical features__\n",
    "\n",
    "Basically we convert categorical colums into numerci codes our model can use directly - think of dummy variables ðŸ˜Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e3a4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value= 1) #this creates the encoder, the parameter ensures unseen categories get -1 at transofrm time.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
