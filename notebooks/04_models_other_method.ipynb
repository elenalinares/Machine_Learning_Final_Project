{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "215db92a",
   "metadata": {},
   "source": [
    "# LightGBM Method\n",
    "\n",
    "__Our third method will be a Light Gradient Boosting Machine__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b067811a",
   "metadata": {},
   "source": [
    "This model was originaly created by Microsoft. It basically is a bunch of many small decision trees, build one after another, where each tree tries to correct the mistakes of the preious ones. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3108f6",
   "metadata": {},
   "source": [
    "We are going to trin a LightGBM model (Poisson) on the dataset,, we will evaluate with Poisson deviance, RMSE/MAE, and show calliration.\n",
    "\n",
    "__First the libraries__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e489e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb #the library of our chosen method\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split #we already have two sets, but with this tool we will get a validation set before the test set\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf891ef",
   "metadata": {},
   "source": [
    "__Now we code the poisson deviance and evaluation metrics__\n",
    "\n",
    "This are just small utility functions, just to help us a bit with this.\n",
    "\n",
    "Because of the nature of the __LightGBM__ method, Poisson deviance becomes a very usefull tool. \n",
    "\n",
    "Counts are different from continous values, Poisson deviance measures how well predicted expected counts exxplain the boserved counts, paying attention to the relative scale - _error when tru counts are large are treated differently than errors when counts are small_.\n",
    "\n",
    "\n",
    "By computing Poisson deviance we can compare our model to what it actually optimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a68a89cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def poisson_deviance(y_true, y_pred, eps=1e-9):\n",
    "    \"\"\"\n",
    "    Compute the mean Poisson deviance between true counts and predicted expected counts.\n",
    "    y_true: observed counts\n",
    "    y_pred: predicted (what we expect) counts (more or equal to 0)\n",
    "    eps    : small value to avoid log(0)\n",
    "    \"\"\"\n",
    "    y_pred = np.maximum(y_pred, eps)\n",
    "    y_true = np.asarray(y_true)\n",
    "    term = y_true * np.log((y_true + eps) / y_pred) - (y_true - y_pred)\n",
    "\n",
    "    return 2.0 * np.mean(term)\n",
    "\n",
    "def evaluate_counts(y_true, y_pred, exposure = None):\n",
    "    \"\"\"\n",
    "    Evaluate predicted counts vs true counts.\n",
    "    IF exposure is provided, predictons/true sould be counts.\n",
    "    Return dict whit RMSE, MAE, Poisson deviance, total observed vs predicted.\n",
    "    \n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"rmse\": np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "        \"mae\": mean_absolute_error(y_true, y_pred),\n",
    "        \"poisson_deviance\": poisson_deviance(y_true, y_pred),\n",
    "        \"total_true\": float(np.sum(y_true)),\n",
    "        \"total_pred\": float(np.sum(y_pred))\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43a5e43",
   "metadata": {},
   "source": [
    "__Then we load and clean the data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ab66191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (542410, 12)\n",
      "Test shape: (135603, 12)\n",
      "\n",
      "Train columns: ['IDpol', 'ClaimNb', 'Exposure', 'Area', 'VehPower', 'VehAge', 'DrivAge', 'BonusMalus', 'VehBrand', 'VehGas', 'Density', 'Region']\n",
      "Rows with Exposure > 1: 994 (0.18% of dataset)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Exposure_orig</th>\n",
       "      <th>Exposure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>1.03</td>\n",
       "      <td>1.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1298</th>\n",
       "      <td>1.01</td>\n",
       "      <td>1.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1767</th>\n",
       "      <td>1.06</td>\n",
       "      <td>1.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990</th>\n",
       "      <td>1.01</td>\n",
       "      <td>1.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2209</th>\n",
       "      <td>1.03</td>\n",
       "      <td>1.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2667</th>\n",
       "      <td>1.03</td>\n",
       "      <td>1.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3380</th>\n",
       "      <td>1.10</td>\n",
       "      <td>1.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4342</th>\n",
       "      <td>1.13</td>\n",
       "      <td>1.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4555</th>\n",
       "      <td>1.05</td>\n",
       "      <td>1.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4844</th>\n",
       "      <td>1.02</td>\n",
       "      <td>1.02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Exposure_orig  Exposure\n",
       "272            1.03      1.03\n",
       "1298           1.01      1.01\n",
       "1767           1.06      1.06\n",
       "1990           1.01      1.01\n",
       "2209           1.03      1.03\n",
       "2667           1.03      1.03\n",
       "3380           1.10      1.10\n",
       "4342           1.13      1.13\n",
       "4555           1.05      1.05\n",
       "4844           1.02      1.02"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with Exposure > 1 (capped): 994\n",
      "Numeric columns: ['VehAge', 'DrivAge', 'BonusMalus', 'Density', 'log_exposure']\n",
      "Categorical columns: ['Area', 'VehPower', 'VehBrand', 'VehGas', 'Region', 'exposure_large']\n",
      "Total features used: 11\n"
     ]
    }
   ],
   "source": [
    "# --- LOAD THE DATA --- #\n",
    "\n",
    "train = pd.read_csv(\"../data/claims_train.csv\")\n",
    "test = pd.read_csv(\"../data/claims_test.csv\")\n",
    "\n",
    "print(\"Train shape:\", train.shape)\n",
    "print(\"Test shape:\", test.shape)\n",
    "print(\"\\nTrain columns:\", train.columns.tolist())\n",
    "\n",
    "train = train.copy() #just in case\n",
    "\n",
    "\n",
    "# --- ABOUT THE EXPOSURE FEATURE --- #\n",
    "\n",
    "#so we got an email from Gabriel that told us how exposure significantly greater than 1 can be suspicious (entry mistakes, policies observed for longer periods or duplaces), so we are capping it at 1, prevvents a few large-exposure rows from skewing the model. \n",
    "train['Exposure_orig'] = train['Exposure']\n",
    "train['exposure_large'] = (train['Exposure'] > 1).astype(int) #new column with the exposures that are above 1\n",
    "\n",
    "num_large = int(train['exposure_large'].sum())\n",
    "print(f\"Rows with Exposure > 1: {num_large} ({num_large/len(train):.2%} of dataset)\")\n",
    "if num_large > 0:\n",
    "    display(train.loc[train['exposure_large'] == 1, ['Exposure_orig', 'Exposure']].head(10))\n",
    "\n",
    "train['Exposure'] = train['Exposure'].clip(upper=1.0)\n",
    "\n",
    "print(\"Rows with Exposure > 1 (capped):\", int(train['exposure_large'].sum()))\n",
    "\n",
    "\n",
    "# --- SELECTING THE DATA --- #\n",
    "\n",
    "target = 'ClaimNb'\n",
    "exposure = 'Exposure'\n",
    "#we are going to exlude both target and exposure as well as ID for the teatures\n",
    "\n",
    "exclude = {target, 'Exposure_orig', exposure, 'IDpol', 'Id', 'PolicyID'}\n",
    "features = [c for c in train.columns if c not in exclude and c != target]#we select the colums we want the model to learn from basically\n",
    "\n",
    "train['log_exposure'] = np.log(train[exposure].replace(0, 1e-6)) #we create a new colum that basically makes exposure a numeric value centered around 0:\n",
    "    # when exposure = 1   --> log_exposure = 0\n",
    "    # when exposure = 0.5 --> log_exposure = -0.693\n",
    "    # when exposure = 2   --> log_exposure = 0.693\n",
    "\n",
    "if 'log_exposure' not in features:\n",
    "    features.append('log_exposure')\n",
    "if 'exposure_large' not in features:\n",
    "    features.append('exposure_large')\n",
    "#we make sure to append those to the features\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#now we just identify the colums this is just to visualize it better dw twin\n",
    "cat_cols = [c for c in features if (train[c].dtype == 'object' or train[c].nunique() <= 50)]\n",
    "num_cols = [c for c in features if c not in cat_cols]\n",
    "\n",
    "# print that shit\n",
    "print(\"Numeric columns:\", num_cols)\n",
    "print(\"Categorical columns:\", cat_cols)\n",
    "print(\"Total features used:\", len(features))\n",
    "\n",
    "\n",
    "#kinda the whole goal of this cell was to see which colums of the table will the model look at and in what form, real long tho, boring as well "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e51b19",
   "metadata": {},
   "source": [
    "__Now, we encode categorical features__\n",
    "\n",
    "Basically we convert categorical colums into numerci codes our model can use directly - think of dummy variables ðŸ˜Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "43e3a4d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Area</th>\n",
       "      <th>VehPower</th>\n",
       "      <th>VehAge</th>\n",
       "      <th>DrivAge</th>\n",
       "      <th>BonusMalus</th>\n",
       "      <th>VehBrand</th>\n",
       "      <th>VehGas</th>\n",
       "      <th>Density</th>\n",
       "      <th>Region</th>\n",
       "      <th>exposure_large</th>\n",
       "      <th>log_exposure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>18</td>\n",
       "      <td>36</td>\n",
       "      <td>95</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1054</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.843970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>17</td>\n",
       "      <td>80</td>\n",
       "      <td>95</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>598</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.302585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3</td>\n",
       "      <td>36</td>\n",
       "      <td>76</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4172</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.108663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4</td>\n",
       "      <td>73</td>\n",
       "      <td>52</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.579818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>50</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3021</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.309333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Area  VehPower  VehAge  DrivAge  BonusMalus  VehBrand  VehGas  Density  \\\n",
       "0   3.0       9.0      18       36          95       0.0     1.0     1054   \n",
       "1   3.0       9.0      17       80          95       6.0     1.0      598   \n",
       "2   4.0       9.0       3       36          76      10.0     1.0     4172   \n",
       "3   0.0       7.0       4       73          52       4.0     0.0       15   \n",
       "4   4.0      10.0       0       37          50       2.0     0.0     3021   \n",
       "\n",
       "   Region  exposure_large  log_exposure  \n",
       "0     4.0             0.0     -0.843970  \n",
       "1     5.0             0.0     -2.302585  \n",
       "2    17.0             0.0     -1.108663  \n",
       "3     4.0             0.0     -0.579818  \n",
       "4    12.0             0.0     -1.309333  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "enc = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value= -1) #this creates the encoder, the parameter ensures unseen categories get -1 at transofrm time\n",
    "\n",
    "if len(cat_cols):\n",
    "    enc.fit(train[cat_cols].astype(str))\n",
    "    train[cat_cols] = enc.transform(train[cat_cols].astype(str))\n",
    "\n",
    "display(train[features].head())\n",
    "#this cell is basically a way better version of the pandas function, way more optimzed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f3b360",
   "metadata": {},
   "source": [
    "__Then, we fit the data inot LightGBM-friendly datasets__\n",
    "\n",
    "So LightGBM uses _lightgbm.Dataset_ for faster training. It's the most suitable form of data for our model, it will allow it to be faster.\n",
    "\n",
    "It's just the best for this method twin, dont even worry bout it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1028d106",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lgb_dataset(X_df, y = None):\n",
    "    \"\"\"\n",
    "    Convert a pandas df (X_df --> features) and optional target y info a lightgbm.Dataset.\n",
    "    It also returns the list of categorical feature indices.\n",
    "    \"\"\"\n",
    "    if not isinstance(X_df, pd.DataFrame): #make sure the provided data is \n",
    "        X_df = pd.DataFrame(X_df, columns = features)\n",
    "    \n",
    "\n",
    "    cat_feature_indices = [X_df.columns.get_loc(c) for c in cat_cols] if len(cat_cols) else []\n",
    "    #LightGBM accepts categorical features as colum indices, we are going to use 'cat_cols' for this\n",
    "\n",
    "    if y is None:\n",
    "        dset = lgb.Dataset(X_df, free_raw_data = False)\n",
    "    else:\n",
    "        dset = lgb.Dataset(X_df, label = y, categorical_feature = cat_feature_indices, free_raw_data = False)\n",
    "    \n",
    "    #this just creates the dataset, if we have y, we include it as labels\n",
    "\n",
    "\n",
    "    return dset, cat_feature_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc43fa6d",
   "metadata": {},
   "source": [
    "__And now, we code teh actual training cell__\n",
    "\n",
    "This is the 5-fold cross_validation loop that actually trains LightGBM, it collects out-of-fold (OOF) predictions for honest evaluation, and prints per-fold metrics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e8ec2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning 5-fold training\n",
      "\n",
      " --- Fold 1 ---\n",
      "Fold 1 results: RMSE = 0.2348, MAE = 0.0964, PoissonDeviance = 0.2935\n",
      "Fold 1 total observed=5884.0, total predicted=5678.0\n",
      "\n",
      " --- Fold 2 ---\n",
      "Fold 2 results: RMSE = 0.2294, MAE = 0.0946, PoissonDeviance = 0.2824\n",
      "Fold 2 total observed=5607.0, total predicted=5745.4\n",
      "\n",
      " --- Fold 3 ---\n",
      "Fold 3 results: RMSE = 0.2371, MAE = 0.0956, PoissonDeviance = 0.2903\n",
      "Fold 3 total observed=5772.0, total predicted=5677.4\n",
      "\n",
      " --- Fold 4 ---\n",
      "Fold 4 results: RMSE = 0.2326, MAE = 0.0955, PoissonDeviance = 0.2881\n",
      "Fold 4 total observed=5722.0, total predicted=5713.7\n",
      "\n",
      " --- Fold 5 ---\n",
      "Fold 5 results: RMSE = 0.2348, MAE = 0.0958, PoissonDeviance = 0.2902\n",
      "Fold 5 total observed=5829.0, total predicted=5670.2\n",
      "\n",
      "Fold metrics summary (mean Â± std):\n",
      "                         mean         std\n",
      "rmse                 0.233712    0.002904\n",
      "mae                  0.095555    0.000634\n",
      "poisson_deviance     0.288891    0.004113\n",
      "total_true        5762.800000  106.177681\n",
      "total_pred        5696.955702   31.972375\n",
      "\n",
      "OOF evaluation on full training set:\n",
      " RMSE: 0.2337\n",
      " MAE: 0.0956\n",
      " Poisson deviance: 0.2889\n",
      " Total observed: 28814.0, Total predicted: 28484.8\n"
     ]
    }
   ],
   "source": [
    "X = train[features].reset_index(drop=True)\n",
    "y = train[target].reset_index(drop = True)\n",
    "#we reset index just in case we get some index alighnemt issues later\n",
    "\n",
    "kf = KFold(n_splits= 5, shuffle= True, random_state= 42)#this splits the training data into 5 different validaiton folds --> every observarion is in validation only once\n",
    "\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"poisson\",     #this tells lightgbm to \"optimize a loss appropiate for count data\"\n",
    "    \"metric\": \"poisson\",        #means \"the model will report poisson emtric durin training\"\n",
    "    \"learning_rate\": 0.05,      #default\n",
    "    \"num_leaves\": 31,           #default\n",
    "    \"min_data_in_leaf\": 20,     #default\n",
    "    \"verbosity\": -1,            #to reduce the spam of training lines\n",
    "    \"seed\": 42                  #this makes the training \"deterministic\" --> makes the model have the same splits, same initial randomness ... = consiestent results\n",
    "}\n",
    "\n",
    "oof_preds = np.zeros(len(X))\n",
    "models = []\n",
    "fold_scores = []\n",
    "\n",
    "print(\"Beginning 5-fold training\")\n",
    "\n",
    "#ok im going to comment a lot here bc this took me a while to understand\n",
    "\n",
    "for fold, (tr_idx, val_idx) in enumerate(kf.split(X,y), start = 1):  \n",
    "   #kf.split separates 4 pairs of index arrays --> train_indices, val_indices; tr_idx (80%) and val_idx (20%) are just arrays of int row indeces referring back to the rows of X and y\n",
    "    print(f'\\n --- Fold {fold} ---')\n",
    "    X_tr, X_val = X.iloc[tr_idx], X.iloc[val_idx]\n",
    "    y_tr, y_val = y.iloc[tr_idx], y.iloc[val_idx]\n",
    "    #we use iloc bc it selects rows by int position --> thats how we select the rows for each fold\n",
    "    #--- It is important that we kee the validation set entirely separate form the fold's training set to avoid data leakage ---\n",
    "\n",
    "    dtrain, cat_idx = make_lgb_dataset(X_tr, y_tr)\n",
    "    dval, _ = make_lgb_dataset(X_val, y_val) \n",
    "    # dtrain / dval --> lightgbm.DAtaset objects, very compact and optimized, great for training\n",
    "    # cat_idx is a list of int column indices that correspond to categorical features --> LightGBM can handl ethem specifically\n",
    "    \n",
    "    \n",
    "    \n",
    "    model = lgb.train(\n",
    "        params,                         #ofc\n",
    "        dtrain,                         #the prepared training dataset for the fold\n",
    "        num_boost_round=2000,           #max number of trees\n",
    "        valid_sets=[dtrain, dval],      #\n",
    "        valid_names=[\"train\", \"valid\"],\n",
    "        #early_stopping_rounds=50,\n",
    "        #verbose_eval=100\n",
    "    )\n",
    "\n",
    "    preds_val = model.predict(X_val, num_iteration = model.best_iteration)\n",
    "    oof_preds[val_idx] = preds_val\n",
    "    models.append(model)\n",
    "\n",
    "    eval_dict = evaluate_counts(y_val.values, preds_val)\n",
    "    fold_scores.append(eval_dict)\n",
    "\n",
    "    print(f'Fold {fold} results: RMSE = {eval_dict['rmse']:.4f}, MAE = {eval_dict['mae']:.4f}, PoissonDeviance = {eval_dict['poisson_deviance']:.4f}')\n",
    "    print(f'Fold {fold} total observed={eval_dict['total_true']:.1f}, total predicted={eval_dict['total_pred']:.1f}')\n",
    "\n",
    "\n",
    "df_fold = pd.DataFrame(fold_scores)\n",
    "print(\"\\nFold metrics summary (mean Â± std):\")\n",
    "print(df_fold.agg(['mean','std']).T[['mean','std']])\n",
    "oof_eval = evaluate_counts(y.values, oof_preds)\n",
    "print(\"\\nOOF evaluation on full training set:\")\n",
    "print(f\" RMSE: {oof_eval['rmse']:.4f}\")\n",
    "print(f\" MAE: {oof_eval['mae']:.4f}\")\n",
    "print(f\" Poisson deviance: {oof_eval['poisson_deviance']:.4f}\")\n",
    "print(f\" Total observed: {oof_eval['total_true']:.1f}, Total predicted: {oof_eval['total_pred']:.1f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
