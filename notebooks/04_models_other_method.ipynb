{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "215db92a",
   "metadata": {},
   "source": [
    "# LightGBM Method\n",
    "\n",
    "__Our third method will be a Light Gradient Boosting Machine__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b067811a",
   "metadata": {},
   "source": [
    "This model was originaly created by Microsoft. It basically is a bunch of many small decision trees, build one after another, where each tree tries to correct the mistakes of the preious ones. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3108f6",
   "metadata": {},
   "source": [
    "We are going to trin a LightGBM model (Poisson) on the dataset,, we will evaluate with Poisson deviance, RMSE/MAE, and show calliration.\n",
    "\n",
    "__First the libraries__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e489e5d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OrdinalEncoder\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mean_squared_error, mean_absolute_error\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m evaluate_counts, poisson_deviance\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjoblib\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "#This model had me tweakin for a bit, i'm going to comment a whole bunch so that i remember everything, i'm going to go crazy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb #the library of our chosen method\n",
    "\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split #we already have two sets, but with this tool we will get a validation set before the test set\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from src.utils import evaluate_counts, poisson_deviance\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf891ef",
   "metadata": {},
   "source": [
    "__Different functions in utils that we will use__\n",
    "\n",
    "This are just small utility functions, just to help us a bit with this.\n",
    "\n",
    "        def evaluate_counts(y_true, y_pred):\n",
    "\n",
    "and\n",
    "\n",
    "        def poisson_deviance(y_true, y_pred, eps=1e-9):\n",
    "\n",
    "Because of the nature of the __LightGBM__ method, Poisson deviance becomes a very usefull tool. \n",
    "\n",
    "Counts are different from continous values, Poisson deviance measures how well predicted expected counts exxplain the boserved counts, paying attention to the relative scale - _error when tru counts are large are treated differently than errors when counts are small_.\n",
    "\n",
    "\n",
    "By computing Poisson deviance we can compare our model to what it actually optimized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43a5e43",
   "metadata": {},
   "source": [
    "__Then we load and clean the data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ab66191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (542410, 12)\n",
      "Test shape: (135603, 12)\n",
      "\n",
      "Train columns: ['IDpol', 'ClaimNb', 'Exposure', 'Area', 'VehPower', 'VehAge', 'DrivAge', 'BonusMalus', 'VehBrand', 'VehGas', 'Density', 'Region']\n",
      "Rows with Exposure > 1: 994 (0.18% of dataset)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Exposure_orig</th>\n",
       "      <th>Exposure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>1.03</td>\n",
       "      <td>1.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1298</th>\n",
       "      <td>1.01</td>\n",
       "      <td>1.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1767</th>\n",
       "      <td>1.06</td>\n",
       "      <td>1.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990</th>\n",
       "      <td>1.01</td>\n",
       "      <td>1.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2209</th>\n",
       "      <td>1.03</td>\n",
       "      <td>1.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2667</th>\n",
       "      <td>1.03</td>\n",
       "      <td>1.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3380</th>\n",
       "      <td>1.10</td>\n",
       "      <td>1.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4342</th>\n",
       "      <td>1.13</td>\n",
       "      <td>1.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4555</th>\n",
       "      <td>1.05</td>\n",
       "      <td>1.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4844</th>\n",
       "      <td>1.02</td>\n",
       "      <td>1.02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Exposure_orig  Exposure\n",
       "272            1.03      1.03\n",
       "1298           1.01      1.01\n",
       "1767           1.06      1.06\n",
       "1990           1.01      1.01\n",
       "2209           1.03      1.03\n",
       "2667           1.03      1.03\n",
       "3380           1.10      1.10\n",
       "4342           1.13      1.13\n",
       "4555           1.05      1.05\n",
       "4844           1.02      1.02"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with Exposure > 1 (capped): 994\n",
      "Numeric columns: ['VehAge', 'DrivAge', 'BonusMalus', 'Density', 'log_exposure']\n",
      "Categorical columns: ['Area', 'VehPower', 'VehBrand', 'VehGas', 'Region', 'exposure_large']\n",
      "Total features used: 11\n"
     ]
    }
   ],
   "source": [
    "# --- LOAD THE DATA --- #\n",
    "\n",
    "train = pd.read_csv(\"../data/claims_train.csv\")\n",
    "test = pd.read_csv(\"../data/claims_test.csv\")\n",
    "\n",
    "print(\"Train shape:\", train.shape)\n",
    "print(\"Test shape:\", test.shape)\n",
    "print(\"\\nTrain columns:\", train.columns.tolist())\n",
    "\n",
    "train = train.copy() #just in case\n",
    "\n",
    "\n",
    "# --- ABOUT THE EXPOSURE FEATURE --- #\n",
    "\n",
    "#so we got an email from Gabriel that told us how exposure significantly greater than 1 can be suspicious (entry mistakes, policies observed for longer periods or duplaces), so we are capping it at 1, prevvents a few large-exposure rows from skewing the model. \n",
    "train['Exposure_orig'] = train['Exposure']\n",
    "train['exposure_large'] = (train['Exposure'] > 1).astype(int) #new column with the exposures that are above 1\n",
    "\n",
    "num_large = int(train['exposure_large'].sum())\n",
    "print(f\"Rows with Exposure > 1: {num_large} ({num_large/len(train):.2%} of dataset)\")\n",
    "if num_large > 0:\n",
    "    display(train.loc[train['exposure_large'] == 1, ['Exposure_orig', 'Exposure']].head(10))\n",
    "\n",
    "train['Exposure'] = train['Exposure'].clip(upper=1.0)\n",
    "\n",
    "print(\"Rows with Exposure > 1 (capped):\", int(train['exposure_large'].sum()))\n",
    "\n",
    "\n",
    "# --- SELECTING THE DATA --- #\n",
    "\n",
    "target = 'ClaimNb'\n",
    "exposure = 'Exposure'\n",
    "#we are going to exlude both target and exposure as well as ID for the teatures\n",
    "\n",
    "exclude = {target, 'Exposure_orig', exposure, 'IDpol', 'Id', 'PolicyID'}\n",
    "features = [c for c in train.columns if c not in exclude and c != target]#we select the colums we want the model to learn from basically\n",
    "\n",
    "train['log_exposure'] = np.log(train[exposure].replace(0, 1e-6)) #we create a new colum that basically makes exposure a numeric value centered around 0:\n",
    "    # when exposure = 1   --> log_exposure = 0\n",
    "    # when exposure = 0.5 --> log_exposure = -0.693\n",
    "    # when exposure = 2   --> log_exposure = 0.693\n",
    "\n",
    "if 'log_exposure' not in features:\n",
    "    features.append('log_exposure')\n",
    "if 'exposure_large' not in features:\n",
    "    features.append('exposure_large')\n",
    "#we make sure to append those to the features\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#now we just identify the colums this is just to visualize it better dw twin\n",
    "cat_cols = [c for c in features if (train[c].dtype == 'object' or train[c].nunique() <= 50)]\n",
    "num_cols = [c for c in features if c not in cat_cols]\n",
    "\n",
    "# print that shit\n",
    "print(\"Numeric columns:\", num_cols)\n",
    "print(\"Categorical columns:\", cat_cols)\n",
    "print(\"Total features used:\", len(features))\n",
    "\n",
    "\n",
    "#kinda the whole goal of this cell was to see which colums of the table will the model look at and in what form, real long tho, boring as well "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e51b19",
   "metadata": {},
   "source": [
    "__Now, we encode categorical features__\n",
    "\n",
    "Basically we convert categorical colums into numerci codes our model can use directly - think of dummy variables ðŸ˜Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43e3a4d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Area</th>\n",
       "      <th>VehPower</th>\n",
       "      <th>VehAge</th>\n",
       "      <th>DrivAge</th>\n",
       "      <th>BonusMalus</th>\n",
       "      <th>VehBrand</th>\n",
       "      <th>VehGas</th>\n",
       "      <th>Density</th>\n",
       "      <th>Region</th>\n",
       "      <th>exposure_large</th>\n",
       "      <th>log_exposure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>18</td>\n",
       "      <td>36</td>\n",
       "      <td>95</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1054</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.843970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>17</td>\n",
       "      <td>80</td>\n",
       "      <td>95</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>598</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.302585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3</td>\n",
       "      <td>36</td>\n",
       "      <td>76</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4172</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.108663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4</td>\n",
       "      <td>73</td>\n",
       "      <td>52</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.579818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>50</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3021</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.309333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Area  VehPower  VehAge  DrivAge  BonusMalus  VehBrand  VehGas  Density  \\\n",
       "0   3.0       9.0      18       36          95       0.0     1.0     1054   \n",
       "1   3.0       9.0      17       80          95       6.0     1.0      598   \n",
       "2   4.0       9.0       3       36          76      10.0     1.0     4172   \n",
       "3   0.0       7.0       4       73          52       4.0     0.0       15   \n",
       "4   4.0      10.0       0       37          50       2.0     0.0     3021   \n",
       "\n",
       "   Region  exposure_large  log_exposure  \n",
       "0     4.0             0.0     -0.843970  \n",
       "1     5.0             0.0     -2.302585  \n",
       "2    17.0             0.0     -1.108663  \n",
       "3     4.0             0.0     -0.579818  \n",
       "4    12.0             0.0     -1.309333  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "enc = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value= -1) #this creates the encoder, the parameter ensures unseen categories get -1 at transofrm time\n",
    "\n",
    "if len(cat_cols):\n",
    "    enc.fit(train[cat_cols].astype(str))\n",
    "    train[cat_cols] = enc.transform(train[cat_cols].astype(str))\n",
    "\n",
    "display(train[features].head())\n",
    "#this cell is basically a way better version of the pandas function, way more optimzed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f3b360",
   "metadata": {},
   "source": [
    "__Then, we fit the data inot LightGBM-friendly datasets__\n",
    "\n",
    "So LightGBM uses _lightgbm.Dataset_ for faster training. It's the most suitable form of data for our model, it will allow it to be faster.\n",
    "\n",
    "It's just the best for this method twin, dont even worry bout it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1028d106",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lgb_dataset(X_df, y = None):\n",
    "    \"\"\"\n",
    "    Convert a pandas df (X_df --> features) and optional target y info a lightgbm.Dataset.\n",
    "    It also returns the list of categorical feature indices.\n",
    "    \"\"\"\n",
    "    if not isinstance(X_df, pd.DataFrame): #make sure the provided data is \n",
    "        X_df = pd.DataFrame(X_df, columns = features)\n",
    "    \n",
    "\n",
    "    cat_feature_indices = [X_df.columns.get_loc(c) for c in cat_cols] if len(cat_cols) else []\n",
    "    #LightGBM accepts categorical features as colum indices, we are going to use 'cat_cols' for this\n",
    "\n",
    "    if y is None:\n",
    "        dset = lgb.Dataset(X_df, free_raw_data = False)\n",
    "    else:\n",
    "        dset = lgb.Dataset(X_df, label = y, categorical_feature = cat_feature_indices, free_raw_data = False)\n",
    "    \n",
    "    #this just creates the dataset, if we have y, we include it as labels\n",
    "\n",
    "\n",
    "    return dset, cat_feature_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc43fa6d",
   "metadata": {},
   "source": [
    "__And now, we code teh actual training cell__\n",
    "\n",
    "This is the 5-fold cross_validation loop that actually trains LightGBM, it collects out-of-fold (OOF) predictions for honest evaluation, and prints per-fold metrics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54e8ec2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning 5-fold training\n",
      "\n",
      " --- Fold 1 ---\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttrain's poisson: 0.192902\tvalid's poisson: 0.198324\n",
      "[200]\ttrain's poisson: 0.190453\tvalid's poisson: 0.197455\n",
      "[300]\ttrain's poisson: 0.188876\tvalid's poisson: 0.197125\n",
      "[400]\ttrain's poisson: 0.187471\tvalid's poisson: 0.196951\n",
      "[500]\ttrain's poisson: 0.186201\tvalid's poisson: 0.196899\n",
      "[600]\ttrain's poisson: 0.185015\tvalid's poisson: 0.196848\n",
      "[700]\ttrain's poisson: 0.183928\tvalid's poisson: 0.196819\n",
      "[800]\ttrain's poisson: 0.182894\tvalid's poisson: 0.196742\n",
      "Early stopping, best iteration is:\n",
      "[831]\ttrain's poisson: 0.18257\tvalid's poisson: 0.196719\n",
      "Fold 1 results: RMSE = 0.2348, MAE = 0.0970, PoissonDeviance = 0.2935\n",
      "Fold 1 total observed=5884.0, total predicted=5721.6\n",
      "\n",
      " --- Fold 2 ---\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttrain's poisson: 0.194926\tvalid's poisson: 0.190881\n",
      "[200]\ttrain's poisson: 0.192395\tvalid's poisson: 0.189751\n",
      "[300]\ttrain's poisson: 0.190799\tvalid's poisson: 0.18947\n",
      "[400]\ttrain's poisson: 0.189479\tvalid's poisson: 0.189338\n",
      "[500]\ttrain's poisson: 0.188241\tvalid's poisson: 0.189222\n",
      "[600]\ttrain's poisson: 0.187081\tvalid's poisson: 0.189156\n",
      "[700]\ttrain's poisson: 0.185963\tvalid's poisson: 0.189075\n",
      "[800]\ttrain's poisson: 0.184932\tvalid's poisson: 0.189057\n",
      "[900]\ttrain's poisson: 0.18391\tvalid's poisson: 0.189006\n",
      "[1000]\ttrain's poisson: 0.18291\tvalid's poisson: 0.188978\n",
      "[1100]\ttrain's poisson: 0.181895\tvalid's poisson: 0.188947\n",
      "Early stopping, best iteration is:\n",
      "[1114]\ttrain's poisson: 0.181746\tvalid's poisson: 0.188937\n",
      "Fold 2 results: RMSE = 0.2291, MAE = 0.0951, PoissonDeviance = 0.2823\n",
      "Fold 2 total observed=5607.0, total predicted=5782.5\n",
      "\n",
      " --- Fold 3 ---\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttrain's poisson: 0.193781\tvalid's poisson: 0.195558\n",
      "[200]\ttrain's poisson: 0.191342\tvalid's poisson: 0.194539\n",
      "[300]\ttrain's poisson: 0.189666\tvalid's poisson: 0.194208\n",
      "[400]\ttrain's poisson: 0.188331\tvalid's poisson: 0.194045\n",
      "[500]\ttrain's poisson: 0.187134\tvalid's poisson: 0.193932\n",
      "[600]\ttrain's poisson: 0.185991\tvalid's poisson: 0.193847\n",
      "[700]\ttrain's poisson: 0.18489\tvalid's poisson: 0.193806\n",
      "[800]\ttrain's poisson: 0.183869\tvalid's poisson: 0.193734\n",
      "Early stopping, best iteration is:\n",
      "[776]\ttrain's poisson: 0.184082\tvalid's poisson: 0.193729\n",
      "Fold 3 results: RMSE = 0.2370, MAE = 0.0964, PoissonDeviance = 0.2904\n",
      "Fold 3 total observed=5772.0, total predicted=5732.7\n",
      "\n",
      " --- Fold 4 ---\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttrain's poisson: 0.193978\tvalid's poisson: 0.194065\n",
      "[200]\ttrain's poisson: 0.191559\tvalid's poisson: 0.193161\n",
      "[300]\ttrain's poisson: 0.18998\tvalid's poisson: 0.192917\n",
      "[400]\ttrain's poisson: 0.188616\tvalid's poisson: 0.192769\n",
      "[500]\ttrain's poisson: 0.187347\tvalid's poisson: 0.192671\n",
      "[600]\ttrain's poisson: 0.186208\tvalid's poisson: 0.192591\n",
      "[700]\ttrain's poisson: 0.185165\tvalid's poisson: 0.19256\n",
      "Early stopping, best iteration is:\n",
      "[701]\ttrain's poisson: 0.185152\tvalid's poisson: 0.192558\n",
      "Fold 4 results: RMSE = 0.2322, MAE = 0.0963, PoissonDeviance = 0.2879\n",
      "Fold 4 total observed=5722.0, total predicted=5776.4\n",
      "\n",
      " --- Fold 5 ---\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttrain's poisson: 0.193503\tvalid's poisson: 0.196476\n",
      "[200]\ttrain's poisson: 0.191085\tvalid's poisson: 0.195396\n",
      "[300]\ttrain's poisson: 0.189552\tvalid's poisson: 0.195046\n",
      "[400]\ttrain's poisson: 0.188235\tvalid's poisson: 0.194891\n",
      "[500]\ttrain's poisson: 0.187007\tvalid's poisson: 0.194766\n",
      "[600]\ttrain's poisson: 0.185908\tvalid's poisson: 0.194741\n",
      "[700]\ttrain's poisson: 0.184786\tvalid's poisson: 0.194708\n",
      "[800]\ttrain's poisson: 0.183726\tvalid's poisson: 0.194674\n",
      "Early stopping, best iteration is:\n",
      "[798]\ttrain's poisson: 0.183743\tvalid's poisson: 0.194674\n",
      "Fold 5 results: RMSE = 0.2348, MAE = 0.0965, PoissonDeviance = 0.2903\n",
      "Fold 5 total observed=5829.0, total predicted=5713.3\n",
      "\n",
      "Fold metrics summary (mean Â± std):\n",
      "                         mean         std\n",
      "rmse                 0.233567    0.003043\n",
      "mae                  0.096275    0.000703\n",
      "poisson_deviance     0.288903    0.004200\n",
      "total_true        5762.800000  106.177681\n",
      "total_pred        5745.283788   31.971710\n",
      "\n",
      "OOF evaluation on full training set:\n",
      " RMSE: 0.2336\n",
      " MAE: 0.0963\n",
      " Poisson deviance: 0.2889\n",
      " Total observed: 28814.0, Total predicted: 28726.4\n"
     ]
    }
   ],
   "source": [
    "X = train[features].reset_index(drop=True)\n",
    "y = train[target].reset_index(drop = True)\n",
    "#we reset index just in case we get some index alighnemt issues later\n",
    "\n",
    "kf = KFold(n_splits= 5, shuffle= True, random_state= 42)#this splits the training data into 5 different validaiton folds --> every observarion is in validation only once\n",
    "\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"poisson\",     #this tells lightgbm to \"optimize a loss appropiate for count data\"\n",
    "    \"metric\": \"poisson\",        #means \"the model will report poisson emtric durin training\"\n",
    "    \"learning_rate\": 0.05,      #default\n",
    "    \"num_leaves\": 31,           #default\n",
    "    \"min_data_in_leaf\": 20,     #default\n",
    "    \"verbosity\": -1,            #to reduce the spam of training lines\n",
    "    \"seed\": 42                  #this makes the training \"deterministic\" --> makes the model have the same splits, same initial randomness ... = consiestent results\n",
    "}\n",
    "\n",
    "oof_preds = np.zeros(len(X))\n",
    "models = []\n",
    "fold_scores = []\n",
    "\n",
    "print(\"Beginning 5-fold training\")\n",
    "\n",
    "#ok im going to comment a lot here bc this took me a while to understand\n",
    "\n",
    "for fold, (tr_idx, val_idx) in enumerate(kf.split(X,y), start = 1):  \n",
    "   #kf.split separates 4 pairs of index arrays --> train_indices, val_indices; tr_idx (80%) and val_idx (20%) are just arrays of int row indeces referring back to the rows of X and y\n",
    "    print(f'\\n --- Fold {fold} ---')\n",
    "    X_tr, X_val = X.iloc[tr_idx], X.iloc[val_idx]\n",
    "    y_tr, y_val = y.iloc[tr_idx], y.iloc[val_idx]\n",
    "    #we use iloc bc it selects rows by int position --> thats how we select the rows for each fold\n",
    "    #--- It is important that we kee the validation set entirely separate form the fold's training set to avoid data leakage ---\n",
    "\n",
    "    dtrain, cat_idx = make_lgb_dataset(X_tr, y_tr)\n",
    "    dval, _ = make_lgb_dataset(X_val, y_val) \n",
    "    # dtrain / dval --> lightgbm.DAtaset objects, very compact and optimized, great for training\n",
    "    # cat_idx is a list of int column indices that correspond to categorical features --> LightGBM can handl ethem specifically\n",
    "    \n",
    "    \n",
    "    \n",
    "    model = lgb.train(\n",
    "        params,                                     #ofc\n",
    "        dtrain,                                     #the prepared training dataset for the fold\n",
    "        num_boost_round=2000,                       #max number of trees\n",
    "        valid_sets=[dtrain, dval],                  #selecting both the training and the evaulation sets\n",
    "        valid_names=[\"train\", \"valid\"],             #the loss of both sets is evaluated - validation loss is key for early stopping and monitoring overfitting\n",
    "        callbacks=[                                 #ok, so this is the best way i could find to have these parameters in the model - newer version of the LightGBM API\n",
    "            lgb.early_stopping(stopping_rounds=50), #this will stop the training after 50 rounds if there has been no improvement \n",
    "            lgb.log_evaluation(period=100)          #prints periodic progress --> every 100 round\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    preds_val = model.predict(X_val, num_iteration = model.best_iteration) #uses the ensemble of trees up to num_iteration to produce predictions for the rosw in X_val; \n",
    "    # num_iterations = model.best_iteration ensures we use the model that had the best validation score\n",
    "    \n",
    "    oof_preds[val_idx] = preds_val #this started as an array of zeros with len(X), but now we insert the predictions for validation rows into their original positions in the full dataset\n",
    "    models.append(model) # we keep track of every model bc we want to be able to keep track of different things from different folds so yeah\n",
    "\n",
    "    eval_dict = evaluate_counts(y_val.values, preds_val) # we call the function we did earlier, this basically returns a ditctionary and will keep track of how good the model performance is\n",
    "    fold_scores.append(eval_dict) #we add every dictionary to a list of dictionarys - to have all the info available\n",
    "\n",
    "    print(f'Fold {fold} results: RMSE = {eval_dict['rmse']:.4f}, MAE = {eval_dict['mae']:.4f}, PoissonDeviance = {eval_dict['poisson_deviance']:.4f}')\n",
    "    print(f'Fold {fold} total observed={eval_dict['total_true']:.1f}, total predicted={eval_dict['total_pred']:.1f}')\n",
    "\n",
    "\n",
    "# I've used it before but i don't think i've explained it but the \":.4f\" of the \":.1f\" just rounds to the 4th and 1st decimal in this case, makes it easier to read\n",
    "\n",
    "df_fold = pd.DataFrame(fold_scores)\n",
    "print(\"\\nFold metrics summary (mean Â± std):\")\n",
    "print(df_fold.agg(['mean','std']).T[['mean','std']])\n",
    "oof_eval = evaluate_counts(y.values, oof_preds)\n",
    "print(\"\\nOOF evaluation on full training set:\")\n",
    "print(f\" RMSE: {oof_eval['rmse']:.4f}\")\n",
    "print(f\" MAE: {oof_eval['mae']:.4f}\")\n",
    "print(f\" Poisson deviance: {oof_eval['poisson_deviance']:.4f}\")\n",
    "print(f\" Total observed: {oof_eval['total_true']:.1f}, Total predicted: {oof_eval['total_pred']:.1f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45937f63",
   "metadata": {},
   "source": [
    "__And finally, I will analyze the results here a bit, more on the report__\n",
    "\n",
    "We get: \n",
    "\n",
    "_RMSE â‰ˆ 0.2336_ --> This means we only miss ~ 0.23 claims, this is very low so _good_ (anything below 0.3 is good)\n",
    "\n",
    "_MAE â‰ˆ 0.0963_ --> On average, the model is off by only ~0.10 claims, so _very good_ as well ðŸ˜Œ (anything that close to 0 is great)\n",
    "\n",
    "_Poisson Deviance â‰ˆ 0.2889_ --> Low as well, means very good fit to count data (0.5 is weak,~0.35 is decent, ~0.25â€“0.30 is strong, <0.20 very rare, really good; ours is strong )\n",
    "\n",
    "_Total observed = 28814_ --> That's the total number of claims\n",
    "\n",
    "_Total Predicted = 28726.4_ --> The total number our model predicted, so it's off only by _0.3%_ (this is the actual calibration, and this one is really good)\n",
    "\n",
    "So our model is a very strong one, generalizes well, doesn't overfit, predicts totals almost perfectly and achieves low RMSE, MAE and Poisson deviance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
